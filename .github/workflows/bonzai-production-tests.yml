name: ðŸš€ Bonzai Production Test Suite
on:
  push:
    branches: [ main, master, development ]
  pull_request:
    branches: [ main, master ]
  schedule:
    # Run tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of test to run'
        required: true
        default: 'full'
        type: choice
        options:
        - full
        - quick
        - readiness
        - orchestration
        - quota-testing

jobs:
  bonzai-health-check:
    name: ðŸ¥ Backend Health Check
    runs-on: ubuntu-latest
    outputs:
      backend-status: ${{ steps.health.outputs.status }}
    steps:
      - name: ðŸ“¥ Checkout Repository
        uses: actions/checkout@v4
        
      - name: ðŸ Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
          
      - name: ðŸ“¦ Install Dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt || echo "No requirements.txt found"
          pip install requests pytest flask python-dotenv anthropic openai google-generativeai mem0
          
      - name: ðŸ¥ Backend Health Check
        id: health
        run: |
          echo "Checking if backend is accessible..."
          python -c "
          import requests
          try:
              response = requests.get('${{ secrets.BONZAI_BACKEND_URL }}/api/health', timeout=10)
              print(f'Backend Status: {response.status_code}')
              if response.status_code == 200:
                  print('âœ… Backend is healthy')
                  print('status=healthy' >> $GITHUB_OUTPUT)
              else:
                  print('âš ï¸ Backend responding but not healthy')
                  print('status=unhealthy' >> $GITHUB_OUTPUT)
          except Exception as e:
              print(f'âŒ Backend unreachable: {e}')
              print('status=unreachable' >> $GITHUB_OUTPUT)
          "

  production-master-tests:
    name: ðŸ§ª Production Master Test Suite
    runs-on: ubuntu-latest
    needs: bonzai-health-check
    if: needs.bonzai-health-check.outputs.backend-status == 'healthy'
    strategy:
      matrix:
        test-category: [
          'ai-models',
          'orchestration', 
          'memory-systems',
          'integrations',
          'performance',
          'security'
        ]
    steps:
      - name: ðŸ“¥ Checkout Repository
        uses: actions/checkout@v4
        
      - name: ðŸ Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
          
      - name: ðŸ“¦ Install Test Dependencies
        run: |
          pip install --upgrade pip
          pip install requests pytest flask python-dotenv anthropic openai google-generativeai mem0
          
      - name: ðŸ§ª Run Production Master Tests
        env:
          # Backend URLs
          BONZAI_BACKEND_URL: ${{ secrets.BONZAI_BACKEND_URL }}
          
          # Core AI Models
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          DEEPSEEK_API_KEY: ${{ secrets.DEEPSEEK_API_KEY }}
          
          # Multiple Gemini Keys for Quota Testing
          GEMINI_API_KEY_1: ${{ secrets.GEMINI_API_KEY_1 }}
          GEMINI_API_KEY_2: ${{ secrets.GEMINI_API_KEY_2 }}
          GEMINI_API_KEY_3: ${{ secrets.GEMINI_API_KEY_3 }}
          
          # Specialized Services
          SCRAPYBARA_API_KEY: ${{ secrets.SCRAPYBARA_API_KEY }}
          E2B_API_KEY: ${{ secrets.E2B_API_KEY }}
          MEM0_API_KEY: ${{ secrets.MEM0_API_KEY }}
          
          # Integrations
          PIPEDREAM_API_TOKEN: ${{ secrets.PIPEDREAM_API_TOKEN }}
          GITHUB_PAT: ${{ secrets.GITHUB_PAT }}
          
        run: |
          echo "ðŸ§ª Running Production Master Test Suite - Category: ${{ matrix.test-category }}"
          python PRODUCTION_MASTER_TEST_SUITE.py --category=${{ matrix.test-category }} --output=json --full-integration
          
      - name: ðŸ“Š Upload Test Results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: production-test-results-${{ matrix.test-category }}
          path: |
            **/test-results-*.json
            **/test-report-*.html
            
  orchestration-stress-test:
    name: ðŸŽ¼ Multi-Model Orchestration Stress Test
    runs-on: ubuntu-latest
    needs: bonzai-health-check
    if: needs.bonzai-health-check.outputs.backend-status == 'healthy' && (github.event.inputs.test_type == 'orchestration' || github.event.inputs.test_type == 'full')
    steps:
      - name: ðŸ“¥ Checkout Repository
        uses: actions/checkout@v4
        
      - name: ðŸ Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: ðŸ“¦ Install Dependencies
        run: |
          pip install --upgrade pip
          pip install requests pytest flask python-dotenv anthropic openai google-generativeai mem0
          
      - name: ðŸŽ¼ Test Multi-Model Orchestration
        env:
          BONZAI_BACKEND_URL: ${{ secrets.BONZAI_BACKEND_URL }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          DEEPSEEK_API_KEY: ${{ secrets.DEEPSEEK_API_KEY }}
          SCRAPYBARA_API_KEY: ${{ secrets.SCRAPYBARA_API_KEY }}
          E2B_API_KEY: ${{ secrets.E2B_API_KEY }}
        run: |
          echo "ðŸŽ¼ Testing multi-model orchestration with all providers..."
          python -c "
          import requests
          import json
          import time
          
          models_to_test = [
              'gemini-2.0-flash-exp',
              'claude-3-5-sonnet',
              'gpt-4',
              'deepseek-chat',
              'auto'  # Test auto-routing
          ]
          
          orchestration_tests = [
              {'test': 'Simple Chat', 'message': 'Say ORCHESTRATION_TEST_PASS and nothing else'},
              {'test': 'Function Calling', 'message': 'Test function calling capability', 'expect_tools': True},
              {'test': 'Computer Use', 'message': 'Use computer tools if available', 'expect_cua': True},
              {'test': 'Memory Integration', 'message': 'Remember this: test_memory_12345', 'expect_memory': True},
              {'test': 'ScrapyBara Test', 'message': 'Scrape a simple webpage if possible', 'expect_scraping': True}
          ]
          
          results = {}
          
          for model in models_to_test:
              model_results = {}
              for test in orchestration_tests:
                  try:
                      start_time = time.time()
                      response = requests.post(
                          '${{ secrets.BONZAI_BACKEND_URL }}/api/chat/simple',
                          json={
                              'model': model,
                              'message': test['message'],
                              'user_id': 'orchestration_test',
                              'enable_tools': test.get('expect_tools', False),
                              'enable_computer_use': test.get('expect_cua', False)
                          },
                          timeout=45
                      )
                      duration = time.time() - start_time
                      
                      if response.status_code == 200:
                          data = response.json()
                          response_text = data.get('response', '')
                          
                          # Detect real vs mock responses
                          is_mock = any(indicator in response_text for indicator in [
                              'Chat response for:', 'Backend ready', 'Mock', 'AI orchestration for'
                          ])
                          
                          is_real = not is_mock and len(response_text) > 5
                          has_expected = test['test'] == 'Simple Chat' and 'ORCHESTRATION_TEST_PASS' in response_text
                          
                          model_results[test['test']] = {
                              'status': 'REAL_AI' if is_real else 'MOCK_DETECTED',
                              'has_expected': has_expected if test['test'] == 'Simple Chat' else None,
                              'response_time': duration,
                              'response_preview': response_text[:150]
                          }
                      else:
                          model_results[test['test']] = {
                              'status': 'HTTP_ERROR',
                              'error_code': response.status_code,
                              'response_time': duration
                          }
                  except Exception as e:
                      model_results[test['test']] = {
                          'status': 'EXCEPTION',
                          'error': str(e)
                      }
              
              results[model] = model_results
          
          # Print results
          print('ðŸŽ¼ ORCHESTRATION TEST RESULTS:')
          real_responses = 0
          total_tests = 0
          
          for model, tests in results.items():
              print(f'\\n{model}:')
              for test_name, result in tests.items():
                  status = result['status']
                  print(f'  {test_name}: {status}')
                  total_tests += 1
                  if status == 'REAL_AI':
                      real_responses += 1
          
          success_rate = (real_responses / total_tests * 100) if total_tests > 0 else 0
          print(f'\\nðŸŽ¯ ORCHESTRATION SUCCESS RATE: {success_rate:.1f}%')
          print(f'Real AI Responses: {real_responses}/{total_tests}')
          
          # Save results
          with open('orchestration_test_results.json', 'w') as f:
              json.dump({
                  'success_rate': success_rate,
                  'real_responses': real_responses,
                  'total_tests': total_tests,
                  'detailed_results': results
              }, f, indent=2)
          
          # Fail if too many mocks detected
          if success_rate < 50:
              print('âŒ ORCHESTRATION FAILURE: Too many mock responses detected!')
              exit(1)
          else:
              print('âœ… ORCHESTRATION SUCCESS: Real AI responses confirmed!')
          "
          
      - name: ðŸ“Š Upload Orchestration Results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: orchestration-test-results
          path: orchestration_test_results.json

  quota-management-test:
    name: âš¡ Quota Management & Fallback Test
    runs-on: ubuntu-latest
    needs: bonzai-health-check
    if: needs.bonzai-health-check.outputs.backend-status == 'healthy' && (github.event.inputs.test_type == 'quota-testing' || github.event.inputs.test_type == 'full')
    steps:
      - name: ðŸ“¥ Checkout Repository
        uses: actions/checkout@v4
        
      - name: ðŸ Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: ðŸ“¦ Install Dependencies
        run: |
          pip install requests python-dotenv
          
      - name: âš¡ Test Quota Management
        env:
          BONZAI_BACKEND_URL: ${{ secrets.BONZAI_BACKEND_URL }}
          GEMINI_API_KEY_1: ${{ secrets.GEMINI_API_KEY_1 }}
          GEMINI_API_KEY_2: ${{ secrets.GEMINI_API_KEY_2 }}
          GEMINI_API_KEY_3: ${{ secrets.GEMINI_API_KEY_3 }}
        run: |
          echo "âš¡ Testing quota management and API key rotation..."
          python -c "
          import requests
          import time
          
          # Test rapid requests to trigger quota management
          print('Testing quota management with rapid requests...')
          
          for i in range(10):
              try:
                  response = requests.post(
                      '${{ secrets.BONZAI_BACKEND_URL }}/api/chat/simple',
                      json={
                          'model': 'gemini-2.0-flash-exp',
                          'message': f'Quota test {i+1}/10: respond with TEST_PASS_{i+1}',
                          'user_id': 'quota_test'
                      },
                      timeout=30
                  )
                  
                  if response.status_code == 200:
                      data = response.json()
                      response_text = data.get('response', '')
                      print(f'Request {i+1}: SUCCESS - {response_text[:50]}')
                  else:
                      print(f'Request {i+1}: HTTP {response.status_code}')
                      
                  time.sleep(1)  # Small delay between requests
                  
              except Exception as e:
                  print(f'Request {i+1}: ERROR - {e}')
          
          print('âœ… Quota management test completed')
          "

  production-readiness-assessment:
    name: ðŸŽ¯ Production Readiness Assessment
    runs-on: ubuntu-latest
    needs: [bonzai-health-check, production-master-tests, orchestration-stress-test]
    if: always() && needs.bonzai-health-check.outputs.backend-status == 'healthy'
    steps:
      - name: ðŸ“¥ Checkout Repository
        uses: actions/checkout@v4
        
      - name: ðŸ Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: ðŸ“¦ Install Dependencies
        run: |
          pip install --upgrade pip
          pip install requests pytest flask python-dotenv anthropic openai google-generativeai mem0
          
      - name: ðŸ“¥ Download Test Results
        uses: actions/download-artifact@v3
        with:
          path: test-artifacts
          
      - name: ðŸŽ¯ Run Production Readiness Assessment
        env:
          BONZAI_BACKEND_URL: ${{ secrets.BONZAI_BACKEND_URL }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          DEEPSEEK_API_KEY: ${{ secrets.DEEPSEEK_API_KEY }}
          SCRAPYBARA_API_KEY: ${{ secrets.SCRAPYBARA_API_KEY }}
          E2B_API_KEY: ${{ secrets.E2B_API_KEY }}
          MEM0_API_KEY: ${{ secrets.MEM0_API_KEY }}
        run: |
          echo "ðŸŽ¯ Running Production Readiness Assessment with full integration testing..."
          python PRODUCTION_READINESS_ASSESSMENT.py --comprehensive --generate-report --include-orchestration
          
      - name: ðŸ“‹ Create Assessment Summary
        id: assessment
        run: |
          # Parse the assessment results and create a summary
          if [ -f "PRODUCTION_READINESS_REPORT.json" ]; then
            SCORE=$(python -c "import json; data=json.load(open('PRODUCTION_READINESS_REPORT.json')); print(data.get('overall_score', 0))")
            STATUS=$(python -c "import json; data=json.load(open('PRODUCTION_READINESS_REPORT.json')); print(data.get('deployment_status', 'UNKNOWN'))")
            ORCHESTRATION_SCORE=$(python -c "import json; data=json.load(open('PRODUCTION_READINESS_REPORT.json')); print(data.get('orchestration_score', 0))")
            echo "score=$SCORE" >> $GITHUB_OUTPUT
            echo "status=$STATUS" >> $GITHUB_OUTPUT
            echo "orchestration_score=$ORCHESTRATION_SCORE" >> $GITHUB_OUTPUT
          else
            echo "score=0" >> $GITHUB_OUTPUT
            echo "status=FAILED" >> $GITHUB_OUTPUT
            echo "orchestration_score=0" >> $GITHUB_OUTPUT
          fi
          
      - name: ðŸ“Š Upload Assessment Report
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: production-readiness-assessment
          path: |
            PRODUCTION_READINESS_REPORT.*
            PRODUCTION_ASSESSMENT_*.json
            
      - name: ðŸ’¬ Comment Assessment Results
        uses: actions/github-script@v6
        if: github.event_name == 'pull_request'
        with:
          script: |
            const score = '${{ steps.assessment.outputs.score }}';
            const status = '${{ steps.assessment.outputs.status }}';
            const orchestrationScore = '${{ steps.assessment.outputs.orchestration_score }}';
            const emoji = status === 'PRODUCTION_READY' ? 'ðŸŽ‰' : status === 'READY_WITH_WARNINGS' ? 'âš ï¸' : 'âŒ';
            
            const comment = `## ${emoji} Bonzai Production Readiness Assessment
            
            **Overall Score:** ${score}/100
            **Orchestration Score:** ${orchestrationScore}/100
            **Deployment Status:** ${status}
            
            ${status === 'PRODUCTION_READY' ? 
              'âœ… **APPROVED FOR BETA DEPLOYMENT**' : 
              status === 'READY_WITH_WARNINGS' ? 
              'âš ï¸ **READY WITH WARNINGS** - Review issues before deployment' :
              'âŒ **NOT READY** - Critical issues must be resolved'
            }
            
            ### ðŸ§ª Test Coverage
            - âœ… Multi-Model Orchestration
            - âœ… ScrapyBara Integration
            - âœ… E2B Virtual Computer
            - âœ… Quota Management
            - âœ… Real AI Response Validation
            
            ðŸ“Š **Full Report:** Check the artifacts for detailed analysis
            ðŸ§ª **Test Results:** All test categories completed with full integration
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

  deployment-gate:
    name: ðŸš¦ Deployment Gate
    runs-on: ubuntu-latest
    needs: [production-readiness-assessment]
    if: always()
    steps:
      - name: ðŸš¦ Deployment Decision
        run: |
          ASSESSMENT_STATUS="${{ needs.production-readiness-assessment.outputs.status }}"
          SCORE="${{ needs.production-readiness-assessment.outputs.score }}"
          ORCHESTRATION_SCORE="${{ needs.production-readiness-assessment.outputs.orchestration_score }}"
          
          echo "ðŸš¦ DEPLOYMENT GATE ANALYSIS"
          echo "Assessment Status: $ASSESSMENT_STATUS"
          echo "Overall Score: $SCORE"
          echo "Orchestration Score: $ORCHESTRATION_SCORE"
          
          if [[ "$ASSESSMENT_STATUS" == "PRODUCTION_READY" ]] && [[ "$ORCHESTRATION_SCORE" -gt "75" ]]; then
            echo "âœ… GATE PASSED - System approved for deployment"
            echo "deployment=approved" >> $GITHUB_OUTPUT
          elif [[ "$ASSESSMENT_STATUS" == "READY_WITH_WARNINGS" ]] && [[ "$SCORE" -gt "80" ]]; then
            echo "âš ï¸ GATE CONDITIONAL - Deployment approved with warnings"
            echo "deployment=conditional" >> $GITHUB_OUTPUT
          else
            echo "âŒ GATE FAILED - System not ready for deployment"
            echo "deployment=blocked" >> $GITHUB_OUTPUT
            exit 1
          fi
