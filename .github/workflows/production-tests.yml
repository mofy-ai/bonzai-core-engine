name: ðŸ§ª Bonzai Backend Production Test Suite
on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run daily at 6 AM UTC
    - cron: '0 6 * * *'
  workflow_dispatch:
    inputs:
      test_level:
        description: 'Test Level'
        required: true
        default: 'full'
        type: choice
        options:
        - quick
        - full
        - stress

env:
  PYTHONPATH: ${{ github.workspace }}
  BACKEND_PORT: 5001

jobs:
  # Quick health check that runs on every commit
  health-check:
    name: ðŸ¥ Backend Health Check
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt || echo "No requirements.txt found"
        pip install pytest requests flask python-dotenv
        
    - name: Create test environment
      run: |
        cp .env.example .env || echo "No .env.example found"
        
    - name: Quick Backend Health Test
      run: |
        python -c "
        import sys, os
        sys.path.insert(0, os.getcwd())
        
        # Quick import test
        try:
            from app import app
            print('âœ… App imports successfully')
        except Exception as e:
            print(f'âŒ App import failed: {e}')
            sys.exit(1)
        "

  # Full production test suite
  production-tests:
    name: ðŸš€ Production Test Suite
    runs-on: ubuntu-latest
    needs: health-check
    timeout-minutes: 30
    if: github.event_name != 'schedule' || github.event.inputs.test_level != 'quick'
    
    strategy:
      matrix:
        python-version: ['3.10', '3.11']
        
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y curl jq
        
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ matrix.python-version }}-${{ hashFiles('**/requirements*.txt') }}
        
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt || pip install flask flask-cors flask-socketio requests python-dotenv asyncio
        pip install pytest pytest-asyncio pytest-cov requests-mock
        
    - name: Create test environment
      run: |
        # Create .env for testing (without real API keys)
        cat > .env << EOF
        FLASK_SECRET_KEY=test-secret-key
        BACKEND_PORT=5001
        DEBUG=False
        LOG_LEVEL=INFO
        
        # Mock API keys for testing
        ANTHROPIC_API_KEY=test-anthropic-key
        OPENAI_API_KEY=test-openai-key
        GOOGLE_API_KEY=test-google-key
        GEMINI_API_KEY=test-gemini-key
        MEM0_API_KEY=test-mem0-key
        EOF
        
    - name: Start Backend Server
      run: |
        python app.py &
        BACKEND_PID=$!
        echo "BACKEND_PID=$BACKEND_PID" >> $GITHUB_ENV
        
        # Wait for server to start
        for i in {1..30}; do
          if curl -f http://localhost:5001/api/health >/dev/null 2>&1; then
            echo "âœ… Backend server started successfully"
            break
          fi
          if [ $i -eq 30 ]; then
            echo "âŒ Backend server failed to start"
            exit 1
          fi
          sleep 2
        done
        
    - name: Run Production Test Suite
      run: |
        # Check if production test files exist, create minimal if not
        if [ ! -f "PRODUCTION_MASTER_TEST_SUITE.py" ]; then
          echo "Creating basic production test..."
          cat > test_production_basic.py << 'EOF'
        import requests
        import json
        import time
        import pytest
        
        class TestBonzaiProduction:
            BASE_URL = "http://localhost:5001"
            
            def test_health_endpoint(self):
                """Test basic health endpoint"""
                response = requests.get(f"{self.BASE_URL}/api/health", timeout=10)
                assert response.status_code == 200
                data = response.json()
                assert data.get("status") == "healthy"
                
            def test_root_endpoint(self):
                """Test root endpoint"""
                response = requests.get(f"{self.BASE_URL}/", timeout=10)
                assert response.status_code == 200
                data = response.json()
                assert "service" in data
                assert data["service"] == "Bonzai Backend"
                
            def test_mcp_tools_endpoint(self):
                """Test MCP tools endpoint"""
                response = requests.get(f"{self.BASE_URL}/api/mcp/tools", timeout=10)
                assert response.status_code == 200
                data = response.json()
                assert "tools" in data
                assert "version" in data
                
            def test_simple_chat_endpoint(self):
                """Test simple chat endpoint"""
                payload = {
                    "model": "test-model",
                    "message": "Hello test",
                    "user_id": "test_user"
                }
                response = requests.post(f"{self.BASE_URL}/api/chat/simple", json=payload, timeout=15)
                assert response.status_code == 200
                data = response.json()
                assert data.get("success") == True
                
            def test_multi_model_status(self):
                """Test multi-model status"""
                response = requests.get(f"{self.BASE_URL}/api/multi-model/status", timeout=10)
                assert response.status_code == 200
                data = response.json()
                assert data.get("success") == True
                assert data.get("service") == "multi_model"
        EOF
          python -m pytest test_production_basic.py -v
        else
          echo "Running full production test suite..."
          python PRODUCTION_MASTER_TEST_SUITE.py
        fi
        
    - name: Run Readiness Assessment
      run: |
        if [ -f "PRODUCTION_READINESS_ASSESSMENT.py" ]; then
          python PRODUCTION_READINESS_ASSESSMENT.py
        else
          echo "âš ï¸ Production readiness assessment not found"
          echo "âœ… Basic tests passed - system appears functional"
        fi
        
    - name: Generate Test Report
      if: always()
      run: |
        echo "## ðŸ§ª Bonzai Backend Test Report" > test_report.md
        echo "**Date:** $(date)" >> test_report.md
        echo "**Python Version:** ${{ matrix.python-version }}" >> test_report.md
        echo "**Commit:** ${{ github.sha }}" >> test_report.md
        echo "" >> test_report.md
        
        if [ -f "BONZAI_TEST_RESULTS.json" ]; then
          echo "**Test Results:**" >> test_report.md
          cat BONZAI_TEST_RESULTS.json >> test_report.md
        fi
        
    - name: Upload Test Results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: test-results-python-${{ matrix.python-version }}
        path: |
          test_report.md
          *.json
          *.log
          
    - name: Cleanup
      if: always()
      run: |
        if [ ! -z "$BACKEND_PID" ]; then
          kill $BACKEND_PID || true
        fi

  # Stress testing job (only on manual trigger or schedule)
  stress-tests:
    name: ðŸ”¥ Stress Testing
    runs-on: ubuntu-latest
    needs: production-tests
    timeout-minutes: 45
    if: github.event.inputs.test_level == 'stress' || github.event_name == 'schedule'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt || pip install flask flask-cors flask-socketio requests python-dotenv
        pip install locust pytest-benchmark
        
    - name: Create stress test environment
      run: |
        cat > .env << EOF
        FLASK_SECRET_KEY=stress-test-secret
        BACKEND_PORT=5001
        DEBUG=False
        LOG_LEVEL=WARNING
        EOF
        
    - name: Start Backend for Stress Testing
      run: |
        python app.py &
        BACKEND_PID=$!
        echo "BACKEND_PID=$BACKEND_PID" >> $GITHUB_ENV
        sleep 10
        
    - name: Run Load Tests
      run: |
        # Create basic load test if none exists
        cat > stress_test.py << 'EOF'
        import requests
        import concurrent.futures
        import time
        import json
        
        def test_endpoint(url, payload=None):
            try:
                if payload:
                    response = requests.post(url, json=payload, timeout=5)
                else:
                    response = requests.get(url, timeout=5)
                return response.status_code == 200
            except:
                return False
        
        def run_stress_test():
            base_url = "http://localhost:5001"
            endpoints = [
                (f"{base_url}/api/health", None),
                (f"{base_url}/", None),
                (f"{base_url}/api/mcp/tools", None),
                (f"{base_url}/api/chat/simple", {"model": "test", "message": "stress test", "user_id": "stress"})
            ]
            
            results = []
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
                for endpoint, payload in endpoints:
                    print(f"Testing {endpoint} with 50 concurrent requests...")
                    futures = [executor.submit(test_endpoint, endpoint, payload) for _ in range(50)]
                    successes = sum(1 for f in concurrent.futures.as_completed(futures) if f.result())
                    success_rate = (successes / 50) * 100
                    print(f"Success rate: {success_rate}%")
                    results.append({"endpoint": endpoint, "success_rate": success_rate})
                    
            with open("stress_test_results.json", "w") as f:
                json.dump(results, f, indent=2)
                
            return results
        
        if __name__ == "__main__":
            results = run_stress_test()
            print("Stress test completed!")
        EOF
        
        python stress_test.py
        
    - name: Upload Stress Test Results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: stress-test-results
        path: stress_test_results.json
        
    - name: Cleanup Stress Test
      if: always()
      run: |
        if [ ! -z "$BACKEND_PID" ]; then
          kill $BACKEND_PID || true
        fi

  # Deployment readiness check
  deployment-check:
    name: ðŸš€ Deployment Readiness
    runs-on: ubuntu-latest
    needs: [production-tests]
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Deployment Readiness Assessment
      run: |
        echo "## ðŸš€ Deployment Readiness Report" > deployment_report.md
        echo "**Branch:** ${{ github.ref_name }}" >> deployment_report.md
        echo "**Commit:** ${{ github.sha }}" >> deployment_report.md
        echo "**Date:** $(date)" >> deployment_report.md
        echo "" >> deployment_report.md
        
        # Check for critical files
        echo "### Critical Files Check:" >> deployment_report.md
        files=("app.py" "requirements.txt" ".env.example")
        for file in "${files[@]}"; do
          if [ -f "$file" ]; then
            echo "âœ… $file exists" >> deployment_report.md
          else
            echo "âŒ $file missing" >> deployment_report.md
          fi
        done
        
        echo "" >> deployment_report.md
        echo "### Services Status:" >> deployment_report.md
        echo "Based on test results, the following services are ready for deployment:" >> deployment_report.md
        echo "- âœ… Core Flask Application" >> deployment_report.md
        echo "- âœ… Health Check Endpoints" >> deployment_report.md
        echo "- âœ… MCP Protocol Integration" >> deployment_report.md
        echo "- âœ… Multi-Model Orchestration Framework" >> deployment_report.md
        
        cat deployment_report.md
        
    - name: Upload Deployment Report
      uses: actions/upload-artifact@v3
      with:
        name: deployment-readiness-report
        path: deployment_report.md
